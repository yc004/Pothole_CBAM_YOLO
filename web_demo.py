import sys
import os
import asyncio

# ä¿®å¤ Windows ä¸‹ä¸­æ–‡ä¹±ç é—®é¢˜
if sys.platform.startswith('win'):
    os.system('chcp 65001 >nul')
    # ä¿®å¤ Windows ä¸‹ asyncio æŠ¥é”™ (WinError 10054)
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
    if hasattr(sys.stdout, 'reconfigure'):
        sys.stdout.reconfigure(encoding='utf-8')
    if hasattr(sys.stderr, 'reconfigure'):
        sys.stderr.reconfigure(encoding='utf-8')

import gradio as gr
import cv2
from ultralytics import YOLO
from PIL import Image
import numpy as np
import subprocess
import shutil

# ================= é…ç½® =================
# æƒé‡è·¯å¾„
MODEL_PATH_BASELINE = "Pothole_Baseline_Project/exp_baseline/weights/best.pt"
MODEL_PATH_CBAM = "Pothole_CBAM_Project/exp_cbam/weights/best.pt"

print(f"â³ æ­£åœ¨åŠ è½½æ¨¡å‹...")
try:
    print(f"   - åŠ è½½åŸºçº¿æ¨¡å‹: {MODEL_PATH_BASELINE}")
    model_baseline = YOLO(MODEL_PATH_BASELINE)
    print(f"   - åŠ è½½æ”¹è¿›æ¨¡å‹ (CBAM): {MODEL_PATH_CBAM}")
    model_cbam = YOLO(MODEL_PATH_CBAM)
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print("è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…æ˜¯å¦å·²ç»è¿è¡Œäº† train_pothole.py è¿›è¡Œè®­ç»ƒã€‚")
    sys.exit(1)

def detect_pothole(image):
    """
    æ‰§è¡Œè·¯é¢å‘æ´¼æ£€æµ‹ (å¯¹æ¯”æ¨¡å¼)
    :param image: è¾“å…¥å›¾ç‰‡ (PIL.Image)
    :return: åŸºçº¿ç»“æœå›¾, æ”¹è¿›ç»“æœå›¾, æ£€æµ‹ä¿¡æ¯æ–‡æœ¬
    """
    if image is None:
        return None, None, "è¯·å…ˆä¸Šä¼ å›¾ç‰‡"

    # 1. åŸºçº¿æ¨¡å‹æ¨ç†
    results_baseline = model_baseline.predict(image, conf=0.25)
    res_base = results_baseline[0]
    plot_base_bgr = res_base.plot()
    plot_base_rgb = plot_base_bgr[..., ::-1] # BGR to RGB
    count_base = len(res_base.boxes)

    # 2. æ”¹è¿›æ¨¡å‹æ¨ç†
    results_cbam = model_cbam.predict(image, conf=0.25)
    res_cbam = results_cbam[0]
    plot_cbam_bgr = res_cbam.plot()
    plot_cbam_rgb = plot_cbam_bgr[..., ::-1] # BGR to RGB
    count_cbam = len(res_cbam.boxes)
    
    info = (f"âœ… æ£€æµ‹å®Œæˆï¼\n"
            f"ğŸ”¹ åŸºçº¿æ¨¡å‹æ£€æµ‹åˆ°: {count_base} ä¸ªç›®æ ‡\n"
            f"ğŸ”¸ æ”¹è¿›æ¨¡å‹æ£€æµ‹åˆ°: {count_cbam} ä¸ªç›®æ ‡")
    
    return plot_base_rgb, plot_cbam_rgb, info

def detect_video(video_path):
    """
    å¤„ç†è§†é¢‘æ–‡ä»¶ (å¯¹æ¯”æ¨¡å¼ - åˆå¹¶æ˜¾ç¤º)
    """
    if video_path is None:
        return None, "è¯·ä¸Šä¼ è§†é¢‘"
        
    cap = cv2.VideoCapture(video_path)
    
    # ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    temp_raw_combined = "temp_raw_combined.mp4"
    output_path_combined = "output_video_combined.mp4"
    
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # è¾“å‡ºè§†é¢‘å®½åº¦ä¸ºä¸¤å€ (å·¦å³å¹¶æ’)
    new_width = width * 2
    
    # OpenCV å†™å…¥ä¸´æ—¶æ–‡ä»¶
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(temp_raw_combined, fourcc, fps, (new_width, height))
    
    frame_count = 0
    total_detections_base = 0
    total_detections_cbam = 0
    
    print("ğŸ”„ æ­£åœ¨é€å¸§å¤„ç†è§†é¢‘ (åˆå¹¶æ¨¡å¼)...")
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # 1. Baseline Inference
        results_base = model_baseline.predict(frame, conf=0.25, verbose=False)
        annotated_frame_base = results_base[0].plot()
        total_detections_base += len(results_base[0].boxes)
        
        # æ·»åŠ æ ‡ç­¾
        cv2.putText(annotated_frame_base, "Baseline", (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 
                    1.2, (0, 0, 255), 3, cv2.LINE_AA)

        # 2. CBAM Inference
        results_cbam = model_cbam.predict(frame, conf=0.25, verbose=False)
        annotated_frame_cbam = results_cbam[0].plot()
        total_detections_cbam += len(results_cbam[0].boxes)
        
        # æ·»åŠ æ ‡ç­¾
        cv2.putText(annotated_frame_cbam, "CBAM (Improved)", (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 
                    1.2, (0, 255, 0), 3, cv2.LINE_AA)
        
        # 3. åˆå¹¶ç”»é¢
        combined_frame = np.hstack((annotated_frame_base, annotated_frame_cbam))
        out.write(combined_frame)
        
        frame_count += 1
        if frame_count % 10 == 0:
            print(f"   å·²å¤„ç† {frame_count} å¸§...", end="\r")
        
    cap.release()
    out.release()
    print(f"\nâœ… è§†é¢‘æ¨ç†å®Œæˆï¼Œå…± {frame_count} å¸§ã€‚")
    
    # è½¬ç å‡½æ•°
    def transcode(input_path, output_path):
        if shutil.which("ffmpeg"):
            try:
                print(f"ğŸ”„ æ­£åœ¨è½¬ç ...")
                subprocess.run([
                    "ffmpeg", "-y", 
                    "-i", input_path, 
                    "-c:v", "h264_mf", # Windows ç¡¬ä»¶åŠ é€Ÿ
                    "-b:v", "5M",      
                    "-rate_control", "cbr", 
                    "-f", "mp4", 
                    output_path
                ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                return output_path, True
            except Exception as e:
                print(f"âš ï¸ è½¬ç å¤±è´¥: {e}")
                return input_path, False
        else:
            return input_path, False

    # æ‰§è¡Œè½¬ç 
    final_video, is_transcoded = transcode(temp_raw_combined, output_path_combined)
    
    msg_status = "è½¬ç æˆåŠŸ" if is_transcoded else "æœªè½¬ç (å¯èƒ½æ— æ³•é¢„è§ˆ)"
    if not shutil.which("ffmpeg"):
         msg_status = "æœªå®‰è£…FFmpegï¼Œæ— æ³•é¢„è§ˆï¼Œè¯·ä¸‹è½½è§‚çœ‹"

    info = (f"âœ… è§†é¢‘å¤„ç†å®Œæˆï¼\n"
            f"å…±å¤„ç† {frame_count} å¸§ã€‚\n"
            f"ğŸ”¹ åŸºçº¿æ¨¡å‹ç´¯è®¡æ£€æµ‹: {total_detections_base} æ¬¡\n"
            f"ğŸ”¸ æ”¹è¿›æ¨¡å‹ç´¯è®¡æ£€æµ‹: {total_detections_cbam} æ¬¡\n"
            f"â„¹ï¸ çŠ¶æ€: {msg_status}")

    return final_video, info

import time

# å…¨å±€å˜é‡ç”¨äºè®¡ç®— FPS
prev_time = 0

def detect_webcam(image):
    """
    æ‘„åƒå¤´å®æ—¶æ£€æµ‹ (åŒæ¨¡å‹å¯¹æ¯”)
    :param image: æ‘„åƒå¤´é‡‡é›†çš„å½“å‰å¸§ (RGB numpy array)
    """
    global prev_time
    
    if image is None:
        return None, "ç­‰å¾…æ‘„åƒå¤´è¾“å…¥..."
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # 1. åŸºçº¿æ¨¡å‹æ¨ç†
    results_base = model_baseline.predict(image, conf=0.25, verbose=False)
    res_base = results_base[0]
    plot_base_bgr = res_base.plot()
    count_base = len(res_base.boxes)
    
    # 2. æ”¹è¿›æ¨¡å‹æ¨ç†
    results_cbam = model_cbam.predict(image, conf=0.25, verbose=False)
    res_cbam = results_cbam[0]
    plot_cbam_bgr = res_cbam.plot()
    count_cbam = len(res_cbam.boxes)
    
    # 3. åˆå¹¶ç”»é¢ (å·¦å³å¹¶æ’)
    combined_bgr = np.hstack((plot_base_bgr, plot_cbam_bgr))
    combined_rgb = combined_bgr[..., ::-1]
    
    # 4. è®¡ç®— FPS
    curr_time = time.time()
    # è®¡ç®—ç¬æ—¶ FPS (åŸºäºæœ¬æ¬¡å¤„ç†æ—¶é—´)
    process_time = curr_time - start_time
    fps = 1 / process_time if process_time > 0 else 0
    
    # ä¹Ÿå¯ä»¥ä½¿ç”¨å¹³æ»‘ FPS (åŸºäºå¸§é—´éš”)
    # fps_smooth = 1 / (curr_time - prev_time) if prev_time > 0 else 0
    prev_time = curr_time
    
    # 5. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    info = (f"â±ï¸ å®æ—¶ FPS: {fps:.2f}\n"
            f"âš¡ å¤„ç†è€—æ—¶: {process_time*1000:.1f} ms\n"
            f"----------------------\n"
            f"ğŸ”¹ Baseline æ£€æµ‹ç›®æ ‡: {count_base}\n"
            f"ğŸ”¸ CBAM (æ”¹è¿›) æ£€æµ‹ç›®æ ‡: {count_cbam}")
    
    return combined_rgb, info

# ================= æ„å»ºç•Œé¢ =================
with gr.Blocks(title="è·¯é¢å‘æ´¼æ£€æµ‹æ¨¡å‹å¯¹æ¯”ç³»ç»Ÿ") as demo:
    gr.Markdown("# ğŸ›£ï¸ è·¯é¢å‘æ´¼æ£€æµ‹ç³»ç»Ÿ - æ¨¡å‹æ•ˆæœå¯¹æ¯”")
    gr.Markdown("æœ¬ç³»ç»ŸåŒæ—¶å±•ç¤º **Baseline (åŸºçº¿æ¨¡å‹)** ä¸ **CBAM (æ”¹è¿›æ¨¡å‹)** çš„æ£€æµ‹ç»“æœï¼Œä»¥ä¾¿ç›´è§‚å¯¹æ¯”æ€§èƒ½å·®å¼‚ã€‚")
    
    with gr.Tabs():
        with gr.TabItem("ğŸ“· å›¾ç‰‡å¯¹æ¯”æ£€æµ‹"):
            gr.Markdown("ä¸Šä¼ è·¯é¢ç…§ç‰‡ï¼Œç³»ç»Ÿå°†åˆ†åˆ«ä½¿ç”¨åŸºçº¿æ¨¡å‹å’Œæ”¹è¿›æ¨¡å‹è¿›è¡Œæ£€æµ‹ã€‚")
            with gr.Row():
                with gr.Column(scale=1):
                    input_img = gr.Image(type="pil", label="ä¸Šä¼ åŸå§‹å›¾ç‰‡")
                    run_btn = gr.Button("å¼€å§‹å¯¹æ¯”æ£€æµ‹", variant="primary")
                
                with gr.Column(scale=2):
                    with gr.Row():
                        output_base = gr.Image(type="numpy", label="åŸºçº¿æ¨¡å‹ (Baseline) ç»“æœ")
                        output_cbam = gr.Image(type="numpy", label="æ”¹è¿›æ¨¡å‹ (CBAM) ç»“æœ")
                    output_text = gr.Textbox(label="æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯")
                    
            run_btn.click(fn=detect_pothole, inputs=input_img, outputs=[output_base, output_cbam, output_text])
            
            gr.Examples(
                examples=["datasets/New_pothole_detection.v2i.yolov8/test/images/1_jpg.rf.a9cc87ae30331b83ba2e75fddcf1ebd5.jpg"],
                inputs=input_img
            )
            
        with gr.TabItem("ğŸ¥ è§†é¢‘å¯¹æ¯”æ£€æµ‹"):
            gr.Markdown("ä¸Šä¼ è·¯é¢è§†é¢‘ï¼Œç³»ç»Ÿå°†ç”Ÿæˆ **Baseline (å·¦)** å’Œ **CBAM (å³)** çš„å¹¶æ’å¯¹æ¯”è§†é¢‘ï¼Œæ–¹ä¾¿é€å¸§æ¯”å¯¹æ•ˆæœã€‚")
            with gr.Row():
                with gr.Column(scale=1):
                    input_video = gr.Video(label="ä¸Šä¼ è§†é¢‘")
                    video_btn = gr.Button("å¼€å§‹å¯¹æ¯”å¤„ç†", variant="primary")
                
                with gr.Column(scale=2):
                    # ç»™ Video ç»„ä»¶æ·»åŠ  elem_idï¼Œæ–¹ä¾¿ JS å®šä½
                    output_video_combined = gr.Video(label="å¯¹æ¯”ç»“æœ (å·¦: Baseline | å³: CBAM)", elem_id="video_output")
                    
                    # æ·»åŠ æ’­æ”¾é€Ÿåº¦æ§åˆ¶æ»‘å—
                    speed_slider = gr.Slider(
                        minimum=0.1, 
                        maximum=2.0, 
                        step=0.1, 
                        value=1.0, 
                        label="æ’­æ”¾é€Ÿåº¦ (0.1x - 2.0x)",
                        elem_id="speed_slider"
                    )
                    
                    video_info = gr.Textbox(label="å¤„ç†ä¿¡æ¯")
            
            # æŒ‰é’®ç‚¹å‡»äº‹ä»¶ï¼ˆåç«¯å¤„ç†ï¼‰
            video_btn.click(fn=detect_video, inputs=input_video, outputs=[output_video_combined, video_info])
            
            # æ»‘å—æ”¹å˜äº‹ä»¶ï¼ˆå‰ç«¯ JS å¤„ç†ï¼‰
            # ä½¿ç”¨ JavaScript ç›´æ¥æ§åˆ¶ video å…ƒç´ çš„ playbackRate
            speed_slider.change(
                fn=None, 
                inputs=speed_slider, 
                outputs=None, 
                js="(speed) => { const video = document.querySelector('#video_output video'); if (video) { video.playbackRate = speed; } }"
            )

        with gr.TabItem("ğŸ”´ å®æ—¶é¢„æµ‹"):
            gr.Markdown("ä½¿ç”¨æ‘„åƒå¤´è¿›è¡Œå®æ—¶è·¯é¢å‘æ´¼æ£€æµ‹ã€‚**å·¦ä¾§ï¼šBaseline (åŸºçº¿æ¨¡å‹) | å³ä¾§ï¼šCBAM (æ”¹è¿›æ¨¡å‹)**")
            gr.Markdown("âš ï¸ æ³¨æ„ï¼šåŒæ—¶è¿è¡Œä¸¤ä¸ªæ¨¡å‹å¯èƒ½ä¼šå¯¼è‡´å¸§ç‡è¾ƒä½ï¼Œå…·ä½“å–å†³äºç¡¬ä»¶æ€§èƒ½ã€‚")
            with gr.Row():
                with gr.Column():
                    # Gradio 4.x å…¼å®¹æ€§ä¿®æ”¹: 
                    # 1. source="webcam" -> sources=["webcam"]
                    # 2. ç§»é™¤ streaming=True (é€šè¿‡ .stream() äº‹ä»¶å¤„ç†)
                    input_webcam = gr.Image(sources=["webcam"], label="æ‘„åƒå¤´è¾“å…¥", type="numpy")
                with gr.Column():
                    output_webcam = gr.Image(label="å®æ—¶æ£€æµ‹ç»“æœ (å¯¹æ¯”)")
                    webcam_info = gr.Textbox(label="å®æ—¶æ•°æ®ç›‘æ§", lines=5)
            
            # ä½¿ç”¨ stream äº‹ä»¶å®ç°å®æ—¶æµå¤„ç† (å¦‚æœæ˜¯æ—§ç‰ˆ Gradioï¼Œå¯èƒ½éœ€è¦ç”¨ change)
            # ä¸ºäº†å…¼å®¹æ€§ï¼Œå°è¯•æ£€æµ‹ stream å±æ€§ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨ change (åœ¨ streaming=True æ—¶é€šå¸¸ä¹Ÿæœ‰æ•ˆ)
            # è¿™é‡Œä½¿ç”¨ stream ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ (å¦‚æœæ”¯æŒ)
            try:
                input_webcam.stream(fn=detect_webcam, inputs=input_webcam, outputs=[output_webcam, webcam_info], show_progress="hidden")
            except AttributeError:
                # Fallback for older Gradio versions
                input_webcam.change(fn=detect_webcam, inputs=input_webcam, outputs=[output_webcam, webcam_info], show_progress="hidden")

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ Web æœåŠ¡...")
    # launch(inbrowser=True) ä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    demo.launch(inbrowser=True)
